{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample tweets for hand-labeling based on classification scores\n",
    "\n",
    "Uses classifiers trained on labeled tweets (about a myth vs. not) to filter tweets from March to August 2020 (especially April-May) to only those with a higher probability of being in the minority class than the majority class. This will be used to select tweets for hand-coding that fall into minority classes, which are hard to capture from the first round of ML models. Data source is tweets with hashtags related to Covid-19.\n",
    "\n",
    "Usage: `python3 6_filter_tweets.py -{myth prefix} {directory of classifier} {directory of vectorizer} -t {threshold lower bound} {threshold upper bound, < not <=} -nrand {number of files sampled from} -nrows {number of rows browsed in each file}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, csv, os\n",
    "from datetime import date\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "import gcsfs # for quick loading of data from gcloud\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import emoji\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom function for loading data\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from shared.utils import gcs_read_json_gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_fp = 'gs://COVID_TWEETS_DIR/'\n",
    "\n",
    "# paths for ML model and vectorizer\n",
    "fvg_mod_fp = '../models/tweet_classifier_5G_DT_020422.joblib'\n",
    "fvg_vec_fp = '../models/vectorizer_5G_020422.joblib'\n",
    "\n",
    "n_sample = 180\n",
    "thisday = date.today().strftime(\"%d%m%y\")\n",
    "\n",
    "# output path for sample\n",
    "fvg_sam_fp = f'../data/myth_5G_sample_{str(n_sample)}_{str(thisday)}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets (dirname, \n",
    "                 num_rand_files = 0, \n",
    "                 start_week_num = -1, \n",
    "                 end_week_num = 0, \n",
    "                 levels = 1, \n",
    "                 nrows = 50, \n",
    "                 previous_tweets = [],\n",
    "                 language = 'en'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read latest Twitter data from JSON files. \n",
    "    Options: take random number of files from each folder, \n",
    "    look only at folders for weeks within specified range, and/or \n",
    "    read files in single folder (1 level) or folder of folders (2 levels).\n",
    "    \n",
    "    Args:\n",
    "        dirname: folder in Google Cloud Compute Server where Twitter JSON files live\n",
    "        num_rand_files: number of random files to draw from each folder. Useful when randomly sampling from files with lots of tweets\n",
    "        start_week_num: first week in range (to take tweets from), inclusive\n",
    "        end_week_num: last week in range (to take tweets from), inclusive\n",
    "        levels: number of folders in hierarchy ('dirname/file1, dirname/file2, etc.' = one level)\n",
    "        nrows: number of tweets to read from file\n",
    "        previous_tweets: list of previous samples to remove duplicate tweets\n",
    "        language: filter to only tweets in this language\n",
    "    Returns:\n",
    "        df: DataFrame with loaded in Twitter data\n",
    "    \"\"\"\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem(project=\"PROJECT\", token=\"cloud\")\n",
    "    \n",
    "    if levels == 2: # two levels (multiple folders)\n",
    "        files = []\n",
    "        folders = [folder for folder in fs.ls(dirname) \n",
    "                   if folder != dirname \n",
    "                   and os.path.join(\"gs://\", folder) != dirname] # don't keep duplicate folders = same as dirname\n",
    "        \n",
    "        for folder in folders:\n",
    "            # only process folders where: start_week_num < week_number < end_week_num\n",
    "            week_num = int(folder.split('/')[-1][9:]) # get week number: last part of file path, anything after '2020-week'\n",
    "            if week_num < start_week_num or (end_week_num > 0 and week_num > end_week_num):\n",
    "                continue # skip if not in week range\n",
    "            \n",
    "            # for folders of weeks in desired range, get their files\n",
    "            listf = fs.ls(os.path.join(\"gs://\", folder)) # get list of files in this folder\n",
    "            listf = [re.sub(r\"#\", r\"%23\", file) for file in listf] # in each file name, replace '#' with '%23' so pandas can read it\n",
    "            if num_rand_files > 0:\n",
    "                listf = sample(listf, num_rand_files) # get specified number of random files\n",
    "            files.extend(listf) # add each file to list\n",
    "            files = [x for x in files if x.endswith(\".json\") or x.endswith(\".json.gz\")]\n",
    "            \n",
    "    else: # one level (just one folder)\n",
    "        files = fs.ls(dirname)\n",
    "        files = [x for x in files if x.endswith(\".json\") or x.endswith(\".json.gz\")]\n",
    "        if num_rand_files > 0:\n",
    "            files = sample(listf, num_rand_files) # get specified number of random files\n",
    "    \n",
    "    print(dirname)\n",
    "    files = [x.replace(\"%23\", \"#\") for x in files]\n",
    "    print(files)\n",
    "    \n",
    "    print(f\"Reading in tweets from {len(files)} JSON files...\")\n",
    "    \n",
    "    # Load and merge files as DFs\n",
    "    dfs = []\n",
    "    for f in tqdm(files):\n",
    "        #print(\"gs://{}\".format(f))\n",
    "        # thisdf = pd.read_json(\"gs://{}\".format(f), nrows = nrows, lines=True, compression = 'gzip')\n",
    "        thisdf = gcs_read_json_gz(\"gs://{}\".format(f), nrows=nrows)\n",
    "        if 'lang' in thisdf.columns:\n",
    "            thisdf['language'] = thisdf['lang'] # funnel 'lang' to 'language' column\n",
    "            thisdf.drop(columns = 'lang', inplace = True) # erase 'lang' column (now a duplicate)\n",
    "        thisdf = thisdf[(thisdf['language'] == 'null') | (thisdf['language']==language)] # Filter to only tweets in language\n",
    "        dfs.append(thisdf)\n",
    "    df = pd.concat(dfs, ignore_index = True)\n",
    "    df['id'] = df['id'].astype(str)\n",
    "    \n",
    "    # Remove duplicate tweets\n",
    "    for id in previous_tweets:\n",
    "        df = df[df['id'] != id] \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ML model(s) for classifying myths\n",
    "fvg_mod = joblib.load(fvg_mod_fp)\n",
    "\n",
    "# Load vectorizer(s) to keep vocab consistent with training data\n",
    "fvg_vec = joblib.load(fvg_vec_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PEOPLE! 5G DOES NOT have anything to do with #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tune into the #IUIC 6Pm est #Sabbath Class tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I see a number of Cell Towers/phone masts have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune into the #IUIC 6Pm est #Sabbath Class tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>rt @crackedscience to implicate 5g cell phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>rt @ianyorston police took piers corbyn away i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>rt @sonsocmed do people really believe the 5g ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>rt @mazinnamdikanu as leaders across the globe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>rt @cgtnofficial how is life like after lockdo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id\n",
       "0                                            full_text\n",
       "1    PEOPLE! 5G DOES NOT have anything to do with #...\n",
       "2    Tune into the #IUIC 6Pm est #Sabbath Class tod...\n",
       "3    I see a number of Cell Towers/phone masts have...\n",
       "4    Tune into the #IUIC 6Pm est #Sabbath Class tod...\n",
       "..                                                 ...\n",
       "262  rt @crackedscience to implicate 5g cell phone ...\n",
       "263  rt @ianyorston police took piers corbyn away i...\n",
       "264  rt @sonsocmed do people really believe the 5g ...\n",
       "265  rt @mazinnamdikanu as leaders across the globe...\n",
       "266  rt @cgtnofficial how is life like after lockdo...\n",
       "\n",
       "[267 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load previous samples for duplicate check\n",
    "\n",
    "previous_fp = ['../data/samples/5G/myth_5G_sample_130.csv',\n",
    "               '../data/samples/5G/myth_5G_sample_135.csv'\n",
    "              ]\n",
    "previous_dfs = []\n",
    "for fp in previous_fp:\n",
    "    previous_dfs.append(pd.read_csv(fp, names=['id']))\n",
    "previous_ids = pd.concat(previous_dfs, ignore_index = True)\n",
    "previous_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://project_coronavirus/raw/hashtag/\n",
      "['project_coronavirus/raw/hashtag/2020-week14/#æ­¦æ¼¢è‚ºç‚Žç–«æƒ…_20200405.json.gz', 'project_coronavirus/raw/hashtag/2020-week15/#WuhanCoronavirus_20200409.json.gz', 'project_coronavirus/raw/hashtag/2020-week16/#SARI_20200419.json.gz', 'project_coronavirus/raw/hashtag/2020-week17/#æ­¦æ¼¢_20200422.json.gz', 'project_coronavirus/raw/hashtag/2020-week18/#æ­¦æ±‰_20200502.json.gz', 'project_coronavirus/raw/hashtag/2020-week19/#2019nCoV_20200510.json.gz', 'project_coronavirus/raw/hashtag/2020-week20/#2019nCoV_20200519.json.gz', 'project_coronavirus/raw/hashtag/2020-week21/#coronavirusoutbreak_20200529.json.gz', 'project_coronavirus/raw/hashtag/2020-week22/twint_#Covid_19_20200525.json.gz', 'project_coronavirus/raw/hashtag/2021-week14/#corona_20210409.json.gz', 'project_coronavirus/raw/hashtag/2021-week15/#WuhanCoronavirus_20210414.json.gz', 'project_coronavirus/raw/hashtag/2021-week16/#2019nCoV_20210424.json.gz', 'project_coronavirus/raw/hashtag/2021-week17/#viruschina_20210428.json.gz', 'project_coronavirus/raw/hashtag/2021-week18/#Wuhan_20210509.json.gz', 'project_coronavirus/raw/hashtag/2021-week19/#æ­¦æ±‰_20210514.json.gz', 'project_coronavirus/raw/hashtag/2021-week20/#æ­¦æ¼¢_20210520.json.gz', 'project_coronavirus/raw/hashtag/2021-week21/#WuhanPneumonia_20210525.json.gz', 'project_coronavirus/raw/hashtag/2021-week22/#WuhanPneumonia_20210605.json.gz']\n",
      "Reading in tweets from 18 JSON files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:13<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows (tweets) and cols in DF: (9531, 62)\n",
      "\n",
      "Columns in tweets DF:\n",
      " Index(['created_at', 'id', 'id_str', 'full_text', 'truncated',\n",
      "       'display_text_range', 'entities', 'extended_entities', 'metadata',\n",
      "       'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'retweeted_status', 'is_quote_status', 'retweet_count',\n",
      "       'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive',\n",
      "       'language', 'quoted_status_id', 'quoted_status_id_str', 'quoted_status',\n",
      "       'withheld_in_countries', 'conversation_id', 'date', 'time', 'timezone',\n",
      "       'user_id', 'username', 'name', 'tweet', 'mentions', 'urls', 'photos',\n",
      "       'replies_count', 'retweets_count', 'likes_count', 'hashtags',\n",
      "       'cashtags', 'link', 'retweet', 'quote_url', 'video', 'thumbnail',\n",
      "       'near', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to',\n",
      "       'retweet_date', 'translate', 'trans_src', 'trans_dest'],\n",
      "      dtype='object')\n",
      "\n",
      "Example tweet 1:\n",
      " RT @RealJamesWoods: This could indicate that the number of #WuhanCoronaVirus patients is higher than is being reported.  https://t.co/Tu36Câ€¦\n",
      "\n",
      "Example tweet 2:\n",
      " RT @RealJamesWoods: This could indicate that the number of #WuhanCoronaVirus patients is higher than is being reported.  https://t.co/Tu36Câ€¦\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>full_text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>metadata</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>near</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-09 23:59:28+00:00</td>\n",
       "      <td>1248400168547831811</td>\n",
       "      <td>1.248400e+18</td>\n",
       "      <td>RT @RealJamesWoods: This could indicate that t...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [{'text': 'WuhanCoronaVirus', 'in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-09 23:59:12+00:00</td>\n",
       "      <td>1248400097391468545</td>\n",
       "      <td>1.248400e+18</td>\n",
       "      <td>@globaltimesnews Did they test for any local c...</td>\n",
       "      <td>False</td>\n",
       "      <td>[17, 157]</td>\n",
       "      <td>{'hashtags': [{'text': 'WuhanCoronaVirus', 'in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-09 23:59:06+00:00</td>\n",
       "      <td>1248400075300061185</td>\n",
       "      <td>1.248400e+18</td>\n",
       "      <td>RT @RealJamesWoods: This could indicate that t...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [{'text': 'WuhanCoronaVirus', 'in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-09 23:59:00+00:00</td>\n",
       "      <td>1248400047936438274</td>\n",
       "      <td>1.248400e+18</td>\n",
       "      <td>RT @TheHKerESL: @DrTedros Now accusing Taiwan ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-09 23:58:58+00:00</td>\n",
       "      <td>1248400041678475264</td>\n",
       "      <td>1.248400e+18</td>\n",
       "      <td>RT @RealJamesWoods: This could indicate that t...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [{'text': 'WuhanCoronaVirus', 'in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                   id        id_str  \\\n",
       "0 2020-04-09 23:59:28+00:00  1248400168547831811  1.248400e+18   \n",
       "1 2020-04-09 23:59:12+00:00  1248400097391468545  1.248400e+18   \n",
       "2 2020-04-09 23:59:06+00:00  1248400075300061185  1.248400e+18   \n",
       "3 2020-04-09 23:59:00+00:00  1248400047936438274  1.248400e+18   \n",
       "4 2020-04-09 23:58:58+00:00  1248400041678475264  1.248400e+18   \n",
       "\n",
       "                                           full_text truncated  \\\n",
       "0  RT @RealJamesWoods: This could indicate that t...     False   \n",
       "1  @globaltimesnews Did they test for any local c...     False   \n",
       "2  RT @RealJamesWoods: This could indicate that t...     False   \n",
       "3  RT @TheHKerESL: @DrTedros Now accusing Taiwan ...     False   \n",
       "4  RT @RealJamesWoods: This could indicate that t...     False   \n",
       "\n",
       "  display_text_range                                           entities  \\\n",
       "0           [0, 140]  {'hashtags': [{'text': 'WuhanCoronaVirus', 'in...   \n",
       "1          [17, 157]  {'hashtags': [{'text': 'WuhanCoronaVirus', 'in...   \n",
       "2           [0, 140]  {'hashtags': [{'text': 'WuhanCoronaVirus', 'in...   \n",
       "3           [0, 140]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4           [0, 140]  {'hashtags': [{'text': 'WuhanCoronaVirus', 'in...   \n",
       "\n",
       "  extended_entities                                           metadata  \\\n",
       "0               NaN  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "1               NaN  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "2               NaN  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "3               NaN  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "4               NaN  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "\n",
       "                                              source  ...  thumbnail  near  \\\n",
       "0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  ...        NaN   NaN   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...  ...        NaN   NaN   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...  ...        NaN   NaN   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...  ...        NaN   NaN   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...  ...        NaN   NaN   \n",
       "\n",
       "   user_rt_id  user_rt retweet_id reply_to retweet_date translate trans_src  \\\n",
       "0         NaN      NaN        NaN      NaN          NaN       NaN       NaN   \n",
       "1         NaN      NaN        NaN      NaN          NaN       NaN       NaN   \n",
       "2         NaN      NaN        NaN      NaN          NaN       NaN       NaN   \n",
       "3         NaN      NaN        NaN      NaN          NaN       NaN       NaN   \n",
       "4         NaN      NaN        NaN      NaN          NaN       NaN       NaN   \n",
       "\n",
       "   trans_dest  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# April - May\n",
    "df = read_tweets(bucket_fp, \n",
    "                 num_rand_files = 1, \n",
    "                 start_week_num = 14, \n",
    "                 end_week_num = 22, \n",
    "                 levels = 2, \n",
    "                 nrows = 5000,\n",
    "                 previous_tweets = previous_ids,\n",
    "                 language='en')\n",
    "\n",
    "print('Number of rows (tweets) and cols in DF:', str(df.shape))\n",
    "print()\n",
    "print('Columns in tweets DF:\\n', str(df.columns))\n",
    "print()\n",
    "\n",
    "# See examples of two tweets. \n",
    "# Have usernames and URLs already been replaced?\n",
    "print(\"Example tweet 1:\\n\", df['full_text'].iloc[0])\n",
    "print()\n",
    "print(\"Example tweet 2:\\n\", df['full_text'].iloc[10])\n",
    "print()\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Tweet Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9531 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9531/9531 [00:03<00:00, 2503.90it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_tweets(tweet):\n",
    "    '''\n",
    "    Preprocesses raw text of a tweet, skipping any retweets. \n",
    "    Steps: lower-casing; removing punctuation, newlines, URLs, usernames, and emojis;\n",
    "    stripping whitespace, replacing hashtags, and finally, lemmatization.\n",
    "    \n",
    "    args:\n",
    "        tweet: raw text of a tweet\n",
    "    \n",
    "    returns:\n",
    "        string: cleaned tweet text\n",
    "    '''\n",
    "    \n",
    "    # Skip retweets and non-strings\n",
    "    retweet_pattern = r'^RT\\s+' # recognize retweets by starting with 'RT'\n",
    "    if not isinstance(tweet, str) or re.search(retweet_pattern, tweet):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Repair hashtag and remove newline character\n",
    "    # from text_helpers.tweet_text_cleanup\n",
    "    tweet = tweet.replace(\"# \", \"#\")\n",
    "    tweet = tweet.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # remove URLs and @mentions\n",
    "    # Simple regular expression to match URLs starting with `https` or `http`\n",
    "    # More complex regex an be found here: https://mathiasbynens.be/demo/url-regex\n",
    "    url_regex = r\"https?://\\S*\"\n",
    "    # Regex to match mentions\n",
    "    mention_regex = r\"@\\S*\"\n",
    "    tweet = re.sub(url_regex, \"\", tweet)\n",
    "    tweet = re.sub(mention_regex, \"\", tweet)\n",
    "        \n",
    "    # Remove additional white spaces\n",
    "    whitespace_pattern = r'\\s+'\n",
    "    tweet = re.sub(whitespace_pattern, ' ', tweet) # strip whitespaces in between words\n",
    "    tweet = tweet.strip() # strip whitespaces at start & end\n",
    "    \n",
    "    # Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    \n",
    "    # Remove emojis\n",
    "    tweet = emoji.get_emoji_regexp().sub(u'', tweet)\n",
    "    \n",
    "    # Lemmatization\n",
    "    tweet = tweet.split()\n",
    "    tweet = ' '.join([stemmer.lemmatize(word) for word in tweet])\n",
    "    \n",
    "    \n",
    "    return tweet\n",
    "\n",
    "print(\"Preprocessing tweets...\")\n",
    "df['text_cleaned'] = df['full_text'].progress_apply(lambda x: process_tweets(x))\n",
    "df = df[df['text_cleaned']!=''] # Filter to only non-empty text_cleaned tweets\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tweet 1 (cleaned):\n",
      " did they test for any local cases? in a city of 25 million, not a single local case of wuhancoronavirus ? how fortunate. ccpliedpeopledied\n",
      "\n",
      "Example tweet 2 (cleaned):\n",
      " on top of wuhancoronavirus, thailand is now battling forest fire too the fire in northern thailand (chiangmai) started sometime in late march, have caused the northern part of the southeast asian nation to be covered in thick smoke. æ³°åœ‹æ¸…é‚å±±ç«æŒçºŒ3æ˜ŸæœŸ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at two (probably different) tweets post-preprocessing\n",
    "print(\"Example tweet 1 (cleaned):\\n\", df['text_cleaned'].iloc[0])\n",
    "print()\n",
    "print(\"Example tweet 2 (cleaned):\\n\", df['text_cleaned'].iloc[10])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 298/2390 [00:00<00:00, 2978.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing words for counting purposes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2390/2390 [00:00<00:00, 3337.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for preprocessed tweets: 9697\n",
      "20 most frequent words in cleaned tweets:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 1875),\n",
       " ('the', 1809),\n",
       " (',', 1387),\n",
       " ('to', 1373),\n",
       " ('coronavirusoutbreak', 1147),\n",
       " ('coronavirus', 1142),\n",
       " ('and', 1112),\n",
       " ('covid19', 947),\n",
       " ('of', 866),\n",
       " ('new', 854),\n",
       " ('in', 848),\n",
       " ('case', 794),\n",
       " (':', 705),\n",
       " ('a', 679),\n",
       " ('wuhancoronavirus', 674),\n",
       " ('is', 657),\n",
       " ('!', 516),\n",
       " ('total', 513),\n",
       " ('corona', 429),\n",
       " ('for', 420)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out vocab size after cleaning\n",
    "# Add words from each cleaned tweet to empty list:\n",
    "tweet_tokens_cleaned = []\n",
    "print(\"Tokenizing words for counting purposes...\")\n",
    "df['text_cleaned'].progress_apply(lambda x: tweet_tokens_cleaned.extend(word_tokenize(x))) # add each word to tokens list\n",
    "\n",
    "print('Vocabulary size for preprocessed tweets:', str(len(set(tweet_tokens_cleaned))))\n",
    "\n",
    "# Check out most frequent words in preprocessed text\n",
    "freq = Counter(tweet_tokens_cleaned)\n",
    "print('20 most frequent words in cleaned tweets:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1248400097391468545</td>\n",
       "      <td>2020-04-09 23:59:12+00:00</td>\n",
       "      <td>@globaltimesnews Did they test for any local c...</td>\n",
       "      <td>did they test for any local cases? in a city o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1248394237952942086</td>\n",
       "      <td>2020-04-09 23:35:55+00:00</td>\n",
       "      <td>âš¡âš¡âš¡ WHY CHINA ðŸ‡¨ðŸ‡³ IS TO BLAME: Origin of the #W...</td>\n",
       "      <td>why china is to blame: origin of the wuhancoro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1248393691393191949</td>\n",
       "      <td>2020-04-09 23:33:44+00:00</td>\n",
       "      <td>@ElizabetGood @TrumpT1776 Bc faici is saying t...</td>\n",
       "      <td>bc faici is saying the wuhancoronavirus will r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1248393579451408388</td>\n",
       "      <td>2020-04-09 23:33:18+00:00</td>\n",
       "      <td>BofA exec berates traders for working from hom...</td>\n",
       "      <td>bofa exec berates trader for working from home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1248393459049705472</td>\n",
       "      <td>2020-04-09 23:32:49+00:00</td>\n",
       "      <td>Yikes! #FoxNews #COVIDãƒ¼19 #WuhanCoronaVirus ht...</td>\n",
       "      <td>yikes! foxnews covidãƒ¼19 wuhancoronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>1391539489001066499</td>\n",
       "      <td>2021-05-09 23:44:02+00:00</td>\n",
       "      <td>#lockdown #USAexposed #usa  #coronavirus #covi...</td>\n",
       "      <td>lockdown usaexposed usa coronavirus covid19 wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>1391543163353853952</td>\n",
       "      <td>2021-05-09 23:58:38+00:00</td>\n",
       "      <td>@shaditaghavi She looks like the Chinese #Wuha...</td>\n",
       "      <td>she look like the chinese wuhan virus when obs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>1397049581746143232</td>\n",
       "      <td>2021-05-25 04:39:10+00:00</td>\n",
       "      <td>@ShekharGupta Why talk about percentage chacha...</td>\n",
       "      <td>why talk about percentage chacha when you canâ€™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>1401174742774648833</td>\n",
       "      <td>2021-06-05 13:51:05+00:00</td>\n",
       "      <td>@ChineseEmbinUK Obviously #CCP, so, #ChinaViru...</td>\n",
       "      <td>obviously ccp, so, chinavirus or chinazivirus,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>1401174046658613253</td>\n",
       "      <td>2021-06-05 13:48:19+00:00</td>\n",
       "      <td>@XHNews Chilling: Tens of thousands of civilia...</td>\n",
       "      <td>chilling: ten of thousand of civilian and stud...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2390 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                created_at  \\\n",
       "0     1248400097391468545 2020-04-09 23:59:12+00:00   \n",
       "1     1248394237952942086 2020-04-09 23:35:55+00:00   \n",
       "2     1248393691393191949 2020-04-09 23:33:44+00:00   \n",
       "3     1248393579451408388 2020-04-09 23:33:18+00:00   \n",
       "4     1248393459049705472 2020-04-09 23:32:49+00:00   \n",
       "...                   ...                       ...   \n",
       "2385  1391539489001066499 2021-05-09 23:44:02+00:00   \n",
       "2386  1391543163353853952 2021-05-09 23:58:38+00:00   \n",
       "2387  1397049581746143232 2021-05-25 04:39:10+00:00   \n",
       "2388  1401174742774648833 2021-06-05 13:51:05+00:00   \n",
       "2389  1401174046658613253 2021-06-05 13:48:19+00:00   \n",
       "\n",
       "                                              full_text  \\\n",
       "0     @globaltimesnews Did they test for any local c...   \n",
       "1     âš¡âš¡âš¡ WHY CHINA ðŸ‡¨ðŸ‡³ IS TO BLAME: Origin of the #W...   \n",
       "2     @ElizabetGood @TrumpT1776 Bc faici is saying t...   \n",
       "3     BofA exec berates traders for working from hom...   \n",
       "4     Yikes! #FoxNews #COVIDãƒ¼19 #WuhanCoronaVirus ht...   \n",
       "...                                                 ...   \n",
       "2385  #lockdown #USAexposed #usa  #coronavirus #covi...   \n",
       "2386  @shaditaghavi She looks like the Chinese #Wuha...   \n",
       "2387  @ShekharGupta Why talk about percentage chacha...   \n",
       "2388  @ChineseEmbinUK Obviously #CCP, so, #ChinaViru...   \n",
       "2389  @XHNews Chilling: Tens of thousands of civilia...   \n",
       "\n",
       "                                           text_cleaned  \n",
       "0     did they test for any local cases? in a city o...  \n",
       "1     why china is to blame: origin of the wuhancoro...  \n",
       "2     bc faici is saying the wuhancoronavirus will r...  \n",
       "3     bofa exec berates trader for working from home...  \n",
       "4              yikes! foxnews covidãƒ¼19 wuhancoronavirus  \n",
       "...                                                 ...  \n",
       "2385  lockdown usaexposed usa coronavirus covid19 wo...  \n",
       "2386  she look like the chinese wuhan virus when obs...  \n",
       "2387  why talk about percentage chacha when you canâ€™...  \n",
       "2388  obviously ccp, so, chinavirus or chinazivirus,...  \n",
       "2389  chilling: ten of thousand of civilian and stud...  \n",
       "\n",
       "[2390 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up: Filter to key columns, including date of tweet via created_at\n",
    "final_df = df[['id','created_at','full_text','text_cleaned']]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute predictions for each tweet using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2390/2390 [00:03<00:00, 761.88it/s]\n",
      "/home/app59/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>prediction_fvg</th>\n",
       "      <th>prediction_fvg_prob_yes</th>\n",
       "      <th>prediction_fvg_prob_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1248400097391468545</td>\n",
       "      <td>2020-04-09 23:59:12+00:00</td>\n",
       "      <td>@globaltimesnews Did they test for any local c...</td>\n",
       "      <td>did they test for any local cases? in a city o...</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1248394237952942086</td>\n",
       "      <td>2020-04-09 23:35:55+00:00</td>\n",
       "      <td>âš¡âš¡âš¡ WHY CHINA ðŸ‡¨ðŸ‡³ IS TO BLAME: Origin of the #W...</td>\n",
       "      <td>why china is to blame: origin of the wuhancoro...</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1248393691393191949</td>\n",
       "      <td>2020-04-09 23:33:44+00:00</td>\n",
       "      <td>@ElizabetGood @TrumpT1776 Bc faici is saying t...</td>\n",
       "      <td>bc faici is saying the wuhancoronavirus will r...</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1248393579451408388</td>\n",
       "      <td>2020-04-09 23:33:18+00:00</td>\n",
       "      <td>BofA exec berates traders for working from hom...</td>\n",
       "      <td>bofa exec berates trader for working from home...</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1248393459049705472</td>\n",
       "      <td>2020-04-09 23:32:49+00:00</td>\n",
       "      <td>Yikes! #FoxNews #COVIDãƒ¼19 #WuhanCoronaVirus ht...</td>\n",
       "      <td>yikes! foxnews covidãƒ¼19 wuhancoronavirus</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>1391539489001066499</td>\n",
       "      <td>2021-05-09 23:44:02+00:00</td>\n",
       "      <td>#lockdown #USAexposed #usa  #coronavirus #covi...</td>\n",
       "      <td>lockdown usaexposed usa coronavirus covid19 wo...</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>1391543163353853952</td>\n",
       "      <td>2021-05-09 23:58:38+00:00</td>\n",
       "      <td>@shaditaghavi She looks like the Chinese #Wuha...</td>\n",
       "      <td>she look like the chinese wuhan virus when obs...</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>1397049581746143232</td>\n",
       "      <td>2021-05-25 04:39:10+00:00</td>\n",
       "      <td>@ShekharGupta Why talk about percentage chacha...</td>\n",
       "      <td>why talk about percentage chacha when you canâ€™...</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>1401174742774648833</td>\n",
       "      <td>2021-06-05 13:51:05+00:00</td>\n",
       "      <td>@ChineseEmbinUK Obviously #CCP, so, #ChinaViru...</td>\n",
       "      <td>obviously ccp, so, chinavirus or chinazivirus,...</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>1401174046658613253</td>\n",
       "      <td>2021-06-05 13:48:19+00:00</td>\n",
       "      <td>@XHNews Chilling: Tens of thousands of civilia...</td>\n",
       "      <td>chilling: ten of thousand of civilian and stud...</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2390 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                created_at  \\\n",
       "0     1248400097391468545 2020-04-09 23:59:12+00:00   \n",
       "1     1248394237952942086 2020-04-09 23:35:55+00:00   \n",
       "2     1248393691393191949 2020-04-09 23:33:44+00:00   \n",
       "3     1248393579451408388 2020-04-09 23:33:18+00:00   \n",
       "4     1248393459049705472 2020-04-09 23:32:49+00:00   \n",
       "...                   ...                       ...   \n",
       "2385  1391539489001066499 2021-05-09 23:44:02+00:00   \n",
       "2386  1391543163353853952 2021-05-09 23:58:38+00:00   \n",
       "2387  1397049581746143232 2021-05-25 04:39:10+00:00   \n",
       "2388  1401174742774648833 2021-06-05 13:51:05+00:00   \n",
       "2389  1401174046658613253 2021-06-05 13:48:19+00:00   \n",
       "\n",
       "                                              full_text  \\\n",
       "0     @globaltimesnews Did they test for any local c...   \n",
       "1     âš¡âš¡âš¡ WHY CHINA ðŸ‡¨ðŸ‡³ IS TO BLAME: Origin of the #W...   \n",
       "2     @ElizabetGood @TrumpT1776 Bc faici is saying t...   \n",
       "3     BofA exec berates traders for working from hom...   \n",
       "4     Yikes! #FoxNews #COVIDãƒ¼19 #WuhanCoronaVirus ht...   \n",
       "...                                                 ...   \n",
       "2385  #lockdown #USAexposed #usa  #coronavirus #covi...   \n",
       "2386  @shaditaghavi She looks like the Chinese #Wuha...   \n",
       "2387  @ShekharGupta Why talk about percentage chacha...   \n",
       "2388  @ChineseEmbinUK Obviously #CCP, so, #ChinaViru...   \n",
       "2389  @XHNews Chilling: Tens of thousands of civilia...   \n",
       "\n",
       "                                           text_cleaned prediction_fvg  \\\n",
       "0     did they test for any local cases? in a city o...             no   \n",
       "1     why china is to blame: origin of the wuhancoro...             no   \n",
       "2     bc faici is saying the wuhancoronavirus will r...             no   \n",
       "3     bofa exec berates trader for working from home...             no   \n",
       "4              yikes! foxnews covidãƒ¼19 wuhancoronavirus             no   \n",
       "...                                                 ...            ...   \n",
       "2385  lockdown usaexposed usa coronavirus covid19 wo...             no   \n",
       "2386  she look like the chinese wuhan virus when obs...             no   \n",
       "2387  why talk about percentage chacha when you canâ€™...             no   \n",
       "2388  obviously ccp, so, chinavirus or chinazivirus,...             no   \n",
       "2389  chilling: ten of thousand of civilian and stud...             no   \n",
       "\n",
       "      prediction_fvg_prob_yes  prediction_fvg_prob_no  \n",
       "0                         0.0                     1.0  \n",
       "1                         0.0                     1.0  \n",
       "2                         0.0                     1.0  \n",
       "3                         0.0                     1.0  \n",
       "4                         0.0                     1.0  \n",
       "...                       ...                     ...  \n",
       "2385                      0.0                     1.0  \n",
       "2386                      0.0                     1.0  \n",
       "2387                      0.0                     1.0  \n",
       "2388                      0.0                     1.0  \n",
       "2389                      0.0                     1.0  \n",
       "\n",
       "[2390 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_prediction(tweet_text_col, vectorizer_model, class_model):\n",
    "    '''\n",
    "    Predicts the label for an input tweet using a given model trained to classify Covid-19-related myths in tweets. \n",
    "    Uses vectorizer_model to restrict the vocab of the input tweets so it's consistent with vocab in class_model (avoids errors).\n",
    "    \n",
    "    Args:\n",
    "        tweet_text_col: array of preprocessed tweet texts\n",
    "        vectorizer_model: fitted text vectorizer\n",
    "        class_model: trained classification model\n",
    "    Returns:\n",
    "        label: label for tweet_text predicted by model, false for tie\n",
    "        prob: probability for label\n",
    "    '''\n",
    "    \n",
    "    X = vectorizer_model.transform(tweet_text_col)\n",
    "    probabilities = class_model.predict_proba(X)\n",
    "    \n",
    "    label = 'no'\n",
    "    prob_no = probabilities[0][0]\n",
    "    prob_yes = probabilities[0][1]\n",
    "    \n",
    "    # predicted label is one with greater probability\n",
    "    if probabilities[0][0] < probabilities[0][1]:\n",
    "        label = 'yes'\n",
    "        \n",
    "    return label, prob_yes, prob_no\n",
    "\n",
    "final_df[['prediction_fvg','prediction_fvg_prob_yes','prediction_fvg_prob_no']] = final_df['text_cleaned'].progress_apply(lambda x: pd.Series(compute_prediction([x], fvg_vec, fvg_mod)))\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distributions of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold(tweets_to_find, df):\n",
    "    '''\n",
    "    Calculates greatest threshold to find requested number of tweets in a dataframe series\n",
    "    \n",
    "    Args:\n",
    "        tweets_to_find: number of tweets to find\n",
    "        df: df series to search through\n",
    "    Returns:\n",
    "        threshold: minimum value of probability\n",
    "    \n",
    "    '''\n",
    "    threshold = .500\n",
    "    tweets_num = df[df > threshold].size\n",
    "    \n",
    "    while tweets_to_find > tweets_num:\n",
    "        threshold = round(threshold - .001,3)\n",
    "        tweets_num = df[df > threshold].size\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5G Myths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1b9eb00190>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(final_df['prediction_fvg_prob_no'] - final_df['prediction_fvg_prob_yes']).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minority Class (yes) Distribution\n",
    "We're interested in \"yes\" cases here because it's easy to find \"no\" cases. <br>\n",
    "Priority is to make sure our classifier can find \"yes\" cases (whether or not they are in majority in coded data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1ba4d365d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_df['prediction_fvg_prob_no'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% (2390) of cases are above -0.001\n"
     ]
    }
   ],
   "source": [
    "threshold = calculate_threshold(225, final_df['prediction_fvg_prob_yes'])\n",
    "num = len(final_df[final_df['prediction_fvg_prob_yes'] > threshold])\n",
    "prop = (num/len(final_df))*100\n",
    "print(f'{str(round(prop,3))}% ({num}) of cases are above {str(threshold)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select tweets for sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sample of new tweets composed of 90% minority class and 10% majority class.<br>\n",
    "To improve fidelity of model, make the tweets selected majority class fuzzy/unreliable, so model gets better at labeling these.<br>\n",
    "First, filter into new DFs, one for minority class and one for (fuzzy) majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of minority cases selected for each myth:\n",
      "2390 for 5G\n"
     ]
    }
   ],
   "source": [
    "# set thresholds for minority cases for each myth\n",
    "# set these as low as possible to capture ~140 tweets likely to fall in minority class\n",
    "capture_num = 162\n",
    "\n",
    "minority_threshold_fvg = calculate_threshold(capture_num,final_df['prediction_fvg_prob_yes'])\n",
    "\n",
    "# filter using threshold\n",
    "df_fvg_minority = final_df[final_df['prediction_fvg_prob_yes'] > minority_threshold_fvg]\n",
    "\n",
    "# check out results\n",
    "print('Number of minority cases selected for each myth:')\n",
    "\n",
    "print(f'{len(df_fvg_minority)} for 5G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_pred_fuzzy(row, \n",
    "                     pred_labels = ['yes', 'no'], \n",
    "                     myth_labels = ['fvg','msq'],\n",
    "                     upper_threshold=0.3, \n",
    "                     lower_threshold=0.10):\n",
    "    \n",
    "    '''\n",
    "    Checks whether prediction is fuzzy/unreliable. Use this to determine if a tweet is worth hand-coding.\n",
    "    Rationale: by only coding tweets with unreliable labels, we can improve the classifier's ability to detect 'unsure' cases.\n",
    "    \n",
    "    Function focuses on this difference: prob(predicted label) - prob(some other label).\n",
    "    If difference is greater than lower_threshold (minimum for hand-coding of tweet to be possible), \n",
    "    but lesser than upper_threshold (maximum for hand-coding to be necessary), then it IS worth coding, so return True. \n",
    "    If difference is not between these, then we either it can't be reliably coded, or already have a reliable prediction,\n",
    "    so we don't need to hand-code --> return False.\n",
    "\n",
    "    To help select a threshold, ask: To what extent do we want the uncertainty to be, to help inform our sample selection?    \n",
    "    For example, if a tweet is labeled as POS with 90% prob and NEG 10% --> this is very obvious sample, so don't bother coding.\n",
    "    On the other hand, if the predictions are 51% NEG and 49 POS, then we need ppl to label this to update our model. \n",
    "    If our upper_threshold is 20% (0.20), then if a tweet has 61% NEG 39% POS probabilities, we don't choose it. \n",
    "    If some other tweet has prob 59% POS 41% NEG, we do select it for coding.\n",
    "    \n",
    "    Args:\n",
    "        row: row corresponding to tweet, with predictions in format...\n",
    "        pred_labels: labels for probabilities to use--used for naming columns\n",
    "        myth_labels: labels for COVID-19 myths to detect--used for naming columns\n",
    "        upper_threshold: max difference between predicted probs\n",
    "        lower_threshold: min difference between predicted probs\n",
    "    \n",
    "    Returns:\n",
    "        Array: True if tweet should be hand-coded, otherwise False. Array contains determinations for all myths\n",
    "    '''\n",
    "    \n",
    "    worth_coding = []\n",
    "    \n",
    "    for myth in myth_labels:\n",
    "        pred_label = row[f'prediction_{myth}'].strip() # get label of prediction for tweet--must be one of those in possible_labels!\n",
    "        pred_score = float(row[f'prediction_{myth}_prob_{pred_label}']) # get probability of predicted label (probably high)\n",
    "        \n",
    "        for pred in pred_labels: # Look at each label\n",
    "            pred = pred.strip() # clean label text\n",
    "            \n",
    "            if pred != pred_label: # if this label isn't the predicted one...\n",
    "                difference = pred_score - float(row[f'prediction_{myth}_prob_{pred}']) # ...then look at their difference in probability\n",
    "                if lower_threshold <= difference <= upper_threshold:\n",
    "                 # if difference in probs is > lower_threshold but < upper_threshold, then pred is fuzzy and we should code\n",
    "                    worth_coding.append(True) # worth coding\n",
    "                \n",
    "                else: worth_coding.append(False)\n",
    "                    \n",
    "    if len(worth_coding) == 1:\n",
    "        return worth_coding[0]\n",
    "    \n",
    "    else: return worth_coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2390/2390 [00:00<00:00, 44024.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_fvg_fuzzy: (0, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_fvg_fuzzy = final_df[final_df.progress_apply(lambda x: check_pred_fuzzy(x, myth_labels = ['fvg'], upper_threshold = .65), axis=1)]\n",
    "\n",
    "print(f'df_fvg_fuzzy: {df_fvg_fuzzy.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 162/162 [00:00<00:00, 342.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 312.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                created_at  \\\n",
      "0  1248270465996472322 2020-04-09 15:24:05+00:00   \n",
      "1  1266301741517041666 2020-05-29 09:33:56+00:00   \n",
      "2  1386102185084157952 2021-04-24 23:38:08+00:00   \n",
      "3  1391394701950005251 2021-05-09 14:08:42+00:00   \n",
      "4  1248076153606115329 2020-04-09 02:31:57+00:00   \n",
      "\n",
      "                                           full_text  \n",
      "0  @RepTedBudd #WuhanCoronaVirus is a big lesson....  \n",
      "1  This is what the plan was..spread coronavirus ...  \n",
      "2  New article: COVID-19 study found that 0.4% of...  \n",
      "3  #Wuhan originated #COVID19 virus was infact ma...  \n",
      "4  ðŸš¨Impeachment Articles delivered The Same Day t...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def sample_tweets(df, df_minority, df_fuzzy, sample_size=150, prop_maj=.1):\n",
    "    '''\n",
    "    Preliminary script to sample tweets using the previously determined minority and fuzzy df.\n",
    "    Does not take into account distribution of dates.\n",
    "    \n",
    "    Args:\n",
    "        df: original df with tweet id and full text\n",
    "        df_minority: minority df with tweets above threshold\n",
    "        sample_size: number of total tweets to sample\n",
    "        prop_maj: proportion of total tweets to be majority label/fuzzy\n",
    "        \n",
    "    '''\n",
    "    df_sample = pd.DataFrame()\n",
    "    min_size = int(sample_size * (1-prop_maj))\n",
    "    maj_size = sample_size - min_size\n",
    "    \n",
    "    for id in tqdm(df_minority['id'].sample(n=min_size)):\n",
    "        df_sample = df_sample.append(df.loc[df['id'] == id][['id','created_at','full_text']],ignore_index=True)\n",
    "    for id in tqdm(df_fuzzy['id'].sample(n=maj_size)):\n",
    "        df_sample = df_sample.append(df.loc[df['id'] == id][['id','created_at','full_text']],ignore_index=True)\n",
    "    df_sample = df_sample.sample(frac=1).reset_index(drop=True)\n",
    "    return df_sample\n",
    "\n",
    "fvg_sample = sample_tweets(final_df,df_fvg_minority, df_fvg_fuzzy, sample_size = n_sample)\n",
    "print(fvg_sample.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check date distribution and remove column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEYCAYAAABFvq0IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAScElEQVR4nO3dfYzkBX3H8fdXDlRYPeCgK72jrgpqLWe1bNWWYHalprSg0FQrxgewtNe08aFqK2djYhujPU3rQ9P2j4vU8IflRCCFiNUSytLaBvTuQCiePAR5xqd6UBf/UPTbP2YGj719mJ3dmd/vO75fyYWdmd/evLlcPjf725mdyEwkSfU8qekASdJgHHBJKsoBl6SiHHBJKsoBl6SiHHBJKmrDKO/smGOOyampqVHe5RM8+uijHHHEEY3d/1pUboe6/VW7oW571W4YXvuePXu+m5nHLrx+pAM+NTXF7t27R3mXTzA3N8fMzExj978Wlduhbn/VbqjbXrUbhtceEfcsdr2nUCSpKAdckopywCWpKAdckopywCWpKAdckopywCWpKAdckooa6Qt5KpraflXfx96944whlkjSE/kIXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqai+Bjwi3hkRt0bE/0TExRHxlIh4VkTcEBF3RMRnIuKwYcdKkn5qxQGPiM3A24HpzDwJOAQ4B/gw8LHMPBHYD5w/zFBJ0hP1ewplA/DUiNgAHA48BLwCuLR7+0XA2eufJ0layooDnpkPAH8D3EtnuB8B9gAPZ+Zj3cPuBzYPK1KSdLDIzOUPiDgKuAx4HfAw8Nnu5fdn5gndY44HPp+ZWxf5/G3ANoDJycmTd+3ata7/A6sxPz/PxMTEqj7nlgce6fvYrZs3rjapb4O0t0nV/qrdULe9ajcMr312dnZPZk4vvL6f98T8DeAbmfkdgIi4HPh14MiI2NB9FL4FeHCxT87MncBOgOnp6ZyZmRns/2AdzM3Nsdr7P28174n5htX93qsxSHubVO2v2g1126t2w+jb+zkHfi/wsog4PCICOA34GnAt8JruMecCVwwnUZK0mH7Ogd9A55uVe4Fbup+zE7gAeFdE3AlsAi4cYqckaYF+TqGQme8H3r/g6ruAl6x7kSSpL74SU5KKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqai+BjwijoyISyPi6xGxLyJ+LSKOjoirI+KO7n+PGnasJOmn+n0E/gngC5n5fOCXgX3AduCazDwRuKZ7WZI0IisOeEQ8HXg5cCFAZv4wMx8GzgIu6h52EXD2sCIlSQfr5xH4s4HvAJ+KiBsj4pMRcQQwmZkPAXT/+3ND7JQkLRCZufwBEdPA9cApmXlDRHwC+D/gbZl55AHH7c/Mg86DR8Q2YBvA5OTkybt27VrP/lWZn59nYmJiVZ9zywOP9H3s1s0bV5vUt0Ha26Rqf9VuqNtetRuG1z47O7snM6cXXt/PgD8DuD4zp7qXT6VzvvsEYCYzH4qI44C5zHzecr/X9PR07t69e8D/hbWbm5tjZmZmVZ8ztf2qvo+9e8cZqyzq3yDtbVK1v2o31G2v2g3Da4+IRQd8xVMomflN4L6I6I3zacDXgCuBc7vXnQtcsU6tkqQ+bOjzuLcBn46Iw4C7gLfQGf9LIuJ84F7gtcNJlCQtpq8Bz8ybgIMevtN5NC5JaoCvxJSkohxwSSqq33Pgjev32SDDfCaIJLWJj8AlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKKcsAlqSgHXJKK6nvAI+KQiLgxIj7XvfysiLghIu6IiM9ExGHDy5QkLbSaR+DvAPYdcPnDwMcy80RgP3D+eoZJkpbX14BHxBbgDOCT3csBvAK4tHvIRcDZwwiUJC0uMnPlgyIuBf4aeBrwZ8B5wPWZeUL39uOBf83Mkxb53G3ANoDJycmTd+3aNVDoLQ880tdxWzdvXPK2+fl5JiYmhnK/K933Wg3S3iZV+6t2Q932qt0wvPbZ2dk9mTm98PoNK31iRJwJfDsz90TETO/qRQ5d9F+CzNwJ7ASYnp7OmZmZxQ5b0Xnbr+rruLvfsPTvPzc3x2rvv9/7Xem+12qQ9jap2l+1G+q2V+2G0bevOODAKcCrI+K3gacATwc+DhwZERsy8zFgC/Dg8DIlSQuteA48M9+bmVsycwo4B/j3zHwDcC3wmu5h5wJXDK1SknSQtTwP/ALgXRFxJ7AJuHB9kiRJ/ejnFMrjMnMOmOt+fBfwkvVPkiT1w1diSlJRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFeWAS1JRDrgkFbXigEfE8RFxbUTsi4hbI+Id3euPjoirI+KO7n+PGn6uJKlnQx/HPAa8OzP3RsTTgD0RcTVwHnBNZu6IiO3AduCC4aVK0tpMbb+qr+Pu3nHGkEvWx4qPwDPzoczc2/34+8A+YDNwFnBR97CLgLOHFSlJOlhkZv8HR0wB/wGcBNybmUcecNv+zDzoNEpEbAO2AUxOTp68a9eugUJveeCRvo7bunnjkrfNz88zMTExlPtd6b7XapD2NqnaX7Ub6rYPs3s9dmQ5w2qfnZ3dk5nTC6/ve8AjYgK4DvhgZl4eEQ/3M+AHmp6ezt27d68yvWM9vvSZm5tjZmZmKPe70n2v1SDtbVK1v2o31G0fZvewT6EMqz0iFh3wvp6FEhGHApcBn87My7tXfysijuvefhzw7fWKlSStrJ9noQRwIbAvMz96wE1XAud2Pz4XuGL98yRJS+nnWSinAG8CbomIm7rX/QWwA7gkIs4H7gVeO5xESdJiVhzwzPwSEEvcfNr65kiS+uUrMSWpKAdckopywCWpKAdckopywCWpKAdckopywCWpKAdckopywCWpKAdckopywCWpKAdckopywCWpKAdckopywCWpKAdckopywCWpKAdckorq5z0xJUmLmNp+1RMuv3vrY5y34DqAu3ecMZT79xG4JBXlgEtSUQ64JBXlgEtSUQ64JBXlgEtSUQ64JBXlgEtSUQ64JBXlgEtSUQ64JBXlgEtSUQ64JBXlTyOUpAUW/pTBtvIRuCQV5YBLUlEOuCQV5YBLUlEOuCQVtaYBj4jTI+K2iLgzIravV5QkaWUDD3hEHAL8A/BbwAuA10fEC9YrTJK0vLU8An8JcGdm3pWZPwR2AWetT5YkaSWRmYN9YsRrgNMz8w+6l98EvDQz37rguG3Atu7F5wG3DZ67ZscA323w/teicjvU7a/aDXXbq3bD8NqfmZnHLrxyLa/EjEWuO+hfg8zcCexcw/2sm4jYnZnTTXcMonI71O2v2g1126t2w+jb13IK5X7g+AMubwEeXFuOJKlfaxnwrwAnRsSzIuIw4BzgyvXJkiStZOBTKJn5WES8FfgicAjwT5l567qVDUcrTuUMqHI71O2v2g1126t2w4jbB/4mpiSpWb4SU5KKcsAlqSgHXJKKcsAlqaixHfCIOGbB5TdGxN9FxLaIWOxFSK0RER+NiFOa7hhURMxGxN9HxBURcVlE7IiIE5ruWouIKPvMiLa2R8ThEfGeiPjziHhKRJwXEVdGxEciYqLpvuVExAsP+PjQiHhft/1DEXH4yDrG9VkoEbE3M3+l+/H7gFOBfwbOBO7PzHc22beciPgOcA9wLPAZ4OLMvLHZqv5ExA5gErgGOBv4BnA78CfAhzLzsw3mLSsijl7qJuCrmblllD2rUbE9Ii4B7gOeSufHbOwDLgFeBTwjM9/UYN6yFuzL3wKbgE/R+Tu/KTPfPJKOMR7wGzPzxd2P9wKnZuajEXEosDcztzZbuLRee0ScSOcFUufQea79xXTG/PZGA5cREbf0/mwjYgNwXWaeEhFHAf+ZmSc1W7i0iPgxnX84D/wKLbuXN2fmYY2E9aFie0TclJkv6n5F/BBwXGZm9/JXM/OFK/wWjVmwLzcBv5qZPxp1+zi/K/1TI+LFdE4THZKZjwJ0/5B/3GzaihIgM+8APgB8oPsl2+uBzwNtPh3xk4g4OjO/B/w8nX94yMz9bT91BdwFnJaZ9y68ISLua6BnNcq2d0f789l9NNm93PZHlhsj4nfo7MuTM/NHMPr2cR7wh4CPdj/+XkQcl5kPRcQm4LEGu/px0NBl5s3AzcB7R5+zKh8CboyI24DnA38MEBHHAl9tMqwPHweOAg4aQeAjI25ZrYrtuyNiIjPnM/P3e1dGxHOA7zfY1Y/rgFd3P74+IiYz81sR8QxG+JMUx/YUylK6b0Tx5Mz8QdMtS+n9pW66Y1Dd87HPpvPz4h9uukf1RETkz9o4DWCcH4ETERuB04HNdE5LPAh8se2jkpnzVdu7fgw8B3h598vJSu2LiohXZubVTXcsJyKeT+dNVQ78O3NlZu5rNGwZFZt72tA+zk8jfDOwF5gBDgeOAGaBPd3bWsv2Vrqw6YDlRMQFdN4VK4Av0/lpoQFc3Nb3q63Y3NOW9rE9hdI9B/vShY/6us+GuCEzn9tM2cpsb0ZELPXjkAN4RWYeMcqe1YiI24Ff6n0z7YDrDwNuzcwTmylbWsXmnra0j/MplGCRdwgCfsLi7ybUJrY341TgjcDC7z8EnfeAbbOf0HnWzz0Lrj+ue1sbVWzuaUX7OA/4B4G9EfFvdF4sAPALwCvpPDWvzWxvxvXADzLzuoU3dL+yaLM/Ba6JiDt44p/7CcBbl/ysZlVs7mlF+9ieQoHHv2z/TTrfZAg6bwP3xczc32hYH2zXakXEk+h8pXDgn/tXMrO1r3uo2NzThvaxHvCFIuLMzPxc0x2DsL0Zxdu3dd9UvIyKzT1NtP+sDfjjP7+gGtubYftoVWzuaaJ9bJ9GuIS2fxNtObY3w/bRqtjcM/L2n7UB/6OmA9bA9mZUbn9V0wEDqNjcM/L2cX4WyqKvlIqI71d9lZftw1e5faHMvB8gIt6SmZ9quqcfFZt7mmgf20fgbXml1CBsb0bl9hX8VdMBA6jY3DOy9rH9JmZbXik1CNubUbz95qVuAp6bmU8eZU8/Kjb3tKV9nE+htOKVUgOyvRmV2yfpPPd+4XPtA/jv0ef0pWJzTyvax3nAW/FKqQHZ3ozK7Z8DJjLzpoU3RMTc6HP6UrG5pxXtY3sKBdrxSqlB2d6Myu362TPWAy5J42ycn4Xywoi4PiLui4id3Z/P0bvty022rcT2Ztg+WhWbe9rSPrYDDvwj8JfAVuB24EvRea89gEObiuqT7c2wfbQqNve0oz0zx/IXcNOCy7PAHcDLgL1N99nevl+221ytfZyfhRIRsTEzHwHIzGsj4neBy4Cjm01bke3NsH20Kjb3tKJ9nE+hfBj4xQOvyMybgdOAyxsp6p/tzbB9tCo297Si3WehSFJRY/sIPCI2RsSOiPh6RPxv99e+7nVHNt23HNubYftoVWzuaUv72A44cAmdl7nOZOamzNxE5xsN+4HPNlq2MtubYftoVWzuaUX72J5CiYjbMvN5q72tDWxvhu2jVbG5py3t4/wI/J6IeE9ETPauiIjJ6PzI0PuW+bw2sL0Zto9WxeaeVrSP84C/DtgEXBcR+yPie8Acnaf4/F6TYX2wvRm2j1bF5p5WtI/tKRR4/N1VtgDXZ+b8AdefnplfaK5sZbY3w/bRqtjc04r2pl/RNKxfwNuB24B/Ae4Gzjrgtra/yst228e+vWJz29rH+ZWYfwicnJnzETEFXBoRU5n5Cdr/zte2N8P20arY3NOK9nEe8EOy+2VNZt4dETN0/pCfSfv/ctjeDNtHq2JzTyvax/mbmN+MiBf1LnT/sM8EjqHzE8TazPZm2D5aFZt7WtE+tt/EjIgtwGOZ+c1FbjslM/+rgay+2N4M20erYnNPW9rHdsAladyN8ykUSRprDrgkFeWAS1JRDrgkFeWAS1JR/w8/u0eFFnV8+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fvg_sample['created_at'].hist(bins = 30, xrot = 90)\n",
    "fvg_sample.drop(columns = 'created_at', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvg_sample.to_csv(fvg_sam_fp,\\\n",
    "    escapechar='\\\"', \\\n",
    "    quotechar='\\\"',\\\n",
    "    quoting=csv.QUOTE_ALL,\\\n",
    "    index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
